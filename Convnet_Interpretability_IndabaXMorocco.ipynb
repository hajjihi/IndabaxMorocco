{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Convnet_Interpretability_IndabaXMorocco.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "zrlVXohssy4s",
        "SQXKRFHki60v"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hajjihi/IndabaxMorocco/blob/master/Convnet_Interpretability_IndabaXMorocco.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CW4roDBuFr2",
        "colab_type": "text"
      },
      "source": [
        "# **Overview of Interpretability Approaches in Deep Learning** ![Texte alternatif…](https://raw.githubusercontent.com/hajjihi/IndabaxMorocco/master/logoIndabaX.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxakIJISsjqT",
        "colab_type": "text"
      },
      "source": [
        "Author:** Pr Hajji Hicham (PhD in CS)**, School of Geomatic Sciences and Surveying Engineering, IAV H2\n",
        "\n",
        "First presented at: IndabaxMorocco 2019, Deep Learning Conference\n",
        "\n",
        "April 30, 2019\n",
        "\n",
        "Contact: hajjihi@gmail.com, linkedin: https://www.linkedin.com/in/dr-hajji-hicham-6601606/ Twitter: @hajjihi\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cd1pZFSwkQmL",
        "colab_type": "text"
      },
      "source": [
        "Ce code se veut comme un récap de plusieurs approches d'interprétations des ConvNets inplémentées dans des librairies comme Keras-Vis, LIME, Google Lucid, DeepExplain. \n",
        "\n",
        "\n",
        "\n",
        "Il s'inspire des colab et notebooks suivants:\n",
        "- https://github.com/raghakot/keras-vis\n",
        "- https://github.com/marcotcr/lime/\n",
        "- https://github.com/tensorflow/lucid\n",
        "- https://github.com/marcoancona/DeepExplain\n",
        "- https://colab.research.google.com/github/idealo/cnn-exposed\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pk5bLbAC1VQB",
        "colab_type": "text"
      },
      "source": [
        "Colab Execution Config: **Python3 & GPU** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovlcf4NqjB_I",
        "colab_type": "text"
      },
      "source": [
        "#Clone du Github Directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAd0PMWRtgqJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf IndabaxMorocco\n",
        "\n",
        "!git clone https://github.com/hajjihi/IndabaxMorocco.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1t3eO-izOkI",
        "colab_type": "text"
      },
      "source": [
        "# **Exploration de Keras-vis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Va0NHuDIrzJS",
        "colab_type": "text"
      },
      "source": [
        "## ** Installation & parametrage**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrKJTLNYjGmX",
        "colab_type": "text"
      },
      "source": [
        "> Installation des packages nécessaires\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-Ny1BkqjLUO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#installation de Keras-vis pour la visualisation des attributions Saliency, GradCAM, ...\n",
        "# site https://raghakot.github.io/keras-vis/\n",
        "!pip install git+https://github.com/raghakot/keras-vis.git -U\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNSgzLagLwIt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip uninstall matplotlib"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzFRcel6Nnuo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install matplotlib===3.0.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMjLBnhdgNzV",
        "colab_type": "text"
      },
      "source": [
        "### **Utility functions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaDxfub_rJSK",
        "colab_type": "text"
      },
      "source": [
        "### **Import des packages**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwBGM8GVrMI-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import packages\n",
        "import os\n",
        "import pathlib\n",
        "import requests\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from skimage import feature, transform\n",
        "from matplotlib.pyplot import figure\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# import keras dependencies\n",
        "from keras.models import Model\n",
        "from keras.applications import MobileNet as CNN\n",
        "from keras.applications.mobilenet import preprocess_input, decode_predictions\n",
        "from keras.preprocessing import image\n",
        "from keras.utils import to_categorical\n",
        "import keras.backend as K\n",
        "\n",
        "# import specific functions from keras-vis package\n",
        "from vis.utils import utils\n",
        "from vis.visualization import visualize_cam, overlay"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wl--V1cw3kXL",
        "colab_type": "text"
      },
      "source": [
        "### **Import des poids du modèle VGG16**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFpW_xngRwvh",
        "colab_type": "text"
      },
      "source": [
        "![Texte alternatif…](https://image.slidesharecdn.com/fttl-keras-nov2016-161120190859/95/transfer-learning-and-fine-tuning-for-cross-domain-image-classification-with-keras-8-638.jpg?cb=1479669021)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwvxucbL4Ogv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.applications import VGG16\n",
        "from vis.utils import utils\n",
        "from keras import activations\n",
        "\n",
        "# Construire le modèle VGG16 avec les poids Imagenet\n",
        "model = VGG16(weights='imagenet', include_top=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UzrM1XQihqD",
        "colab_type": "text"
      },
      "source": [
        "##Préparation du Modèle et des images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrlVXohssy4s",
        "colab_type": "text"
      },
      "source": [
        "### **Préparation du modèle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-MllwvBilgO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remplacer la fonction softmax de la dernière couche avec une fonction linéaire (softmax pose problème dans le gradient de classe en question)\n",
        "\n",
        "layer_idx = utils.find_layer_idx(model, 'predictions')\n",
        "\n",
        "model.layers[layer_idx].activation = activations.linear\n",
        "model = utils.apply_modifications(model)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDV0qm5wlFpQ",
        "colab_type": "text"
      },
      "source": [
        "### **Préparation des images**#"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WAyF2zClGO1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from vis.utils import utils\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (18, 6)\n",
        "\n",
        "img1 = utils.load_img('IndabaxMorocco/ouzel1.jpg', target_size=(224, 224))\n",
        "#img2 = utils.load_img('IndabaxMorocco/ouzel2.jpg', target_size=(224, 224))\n",
        "\n",
        "f, ax = plt.subplots(1, 1)\n",
        "#f, ax = plt.subplots(1, 2)\n",
        "\n",
        "ax.imshow(img1)\n",
        "\n",
        "#ax[1].imshow(img2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rP9CjGr93n35",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loaded_image = np.array(image.load_img('IndabaxMorocco/ouzel1.jpg', target_size=(224, 224)))\n",
        "processed_image = preprocess_input(loaded_image)\n",
        "preds = model.predict(processed_image[np.newaxis, :])\n",
        "preds_name = decode_predictions(preds, top=5)\n",
        "preds_name"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24U7yjuHueky",
        "colab_type": "text"
      },
      "source": [
        "## Affichage des attributions avec Keras-vis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-Y3i78tPRB2",
        "colab_type": "text"
      },
      "source": [
        "![Texte alternatif…](http://i.imgur.com/bHLF8it.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CYf4YmUiCC-",
        "colab_type": "text"
      },
      "source": [
        "### **Saliency Maps**# \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VrmGiLuiHBg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#voir ici les codes des classes dans imagenet: https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a\n",
        "from vis.visualization import visualize_saliency, overlay\n",
        "from vis.utils import utils\n",
        "from keras import activations\n",
        "\n",
        "layer_idx = utils.find_layer_idx(model, 'predictions')\n",
        "f, ax = plt.subplots(1, 1)\n",
        "ax.imshow(img1)\n",
        "#ax[0].imshow(img1)\n",
        "#ax[1].imshow(img2)\n",
        "f, ax = plt.subplots(1, 1)\n",
        "for i, img in enumerate([img1]):    \n",
        "    # 20 is the imagenet index corresponding to `ouzel`\n",
        "    grads = visualize_saliency(model, layer_idx, filter_indices=20, seed_input=img)\n",
        "    \n",
        "    # visualiser grads comme une heatmap (carte de chaleur)\n",
        "    ax.imshow(grads, cmap='jet')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYfDyZjamWaW",
        "colab_type": "text"
      },
      "source": [
        "### ** Guided Backpropagation**# \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFUyHel4mY5A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Selon le site de Kears.vis:\n",
        "#le paramètre \"guided\": Modifies backprop to only propagate positive gradients for positive activations.\n",
        "#References: Details on guided back propagation can be found in paper: [String For Simplicity: The All Convolutional Net] (https://arxiv.org/pdf/1412.6806.pdf)\n",
        "\n",
        "#le paramètre \"relu\": Modifies backprop to only propagate positive gradients.\n",
        "#References: Details can be found in the paper: [Visualizing and Understanding Convolutional Networks] (https://arxiv.org/pdf/1311.2901.pdf)\n",
        "plt.figure()\n",
        "f, ax = plt.subplots(1, 1)\n",
        "ax.imshow(img1)\n",
        "#ax[1].imshow(img2)\n",
        "for modifier in ['guided']:\n",
        "    f, ax = plt.subplots(1, 1)\n",
        "    plt.suptitle(modifier)\n",
        "    for i, img in enumerate([img1]):    \n",
        "        # 20 is the imagenet index corresponding to `ouzel`\n",
        "        # 669: 'mosquito net',\n",
        "        # 141: 'redshank\n",
        "        grads = visualize_saliency(model, layer_idx, filter_indices=141, \n",
        "                                   seed_input=img, backprop_modifier=modifier)\n",
        "        # Lets overlay the heatmap onto original image.    \n",
        "\n",
        "        ax.imshow(grads, cmap='jet')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKOG_sFeiLjH",
        "colab_type": "text"
      },
      "source": [
        "### ** Grad-CAM** \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhiOV3UOPjFg",
        "colab_type": "text"
      },
      "source": [
        "![Texte alternatif…](https://camo.githubusercontent.com/450498bd998fd99d51b647d2b6c8631e94585522/687474703a2f2f692e696d6775722e636f6d2f4a614762645a352e706e67)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54S4NqaVuTNi",
        "colab_type": "text"
      },
      "source": [
        "*** Affichage du Modèle***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcPpTyv_uVnc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4ZX_kuIiM3C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.cm as cm\n",
        "from vis.visualization import visualize_cam\n",
        "plt.figure()\n",
        "f, ax = plt.subplots(1, 1)\n",
        "ax.imshow(img1)\n",
        "#ax[1].imshow(img2)\n",
        "for modifier in ['guided']:\n",
        "    f, ax = plt.subplots(1, 1)\n",
        "    plt.suptitle(\"vanilla\" if modifier is None else modifier)\n",
        "    for i, img in enumerate([img1]):    \n",
        "        # 20 is the imagenet index corresponding to `ouzel`\n",
        "        grads = visualize_cam(model, layer_idx, filter_indices=20, \n",
        "                              seed_input=img, backprop_modifier=modifier)        \n",
        "        # Lets overlay the heatmap onto original image.    \n",
        "        jet_heatmap = np.uint8(cm.jet(grads)[..., :3] * 255)\n",
        "                \n",
        "        #print(grads.shape) \n",
        "        \n",
        "        ax.imshow(overlay(jet_heatmap, img))\n",
        "        #ax[i].imshow(overlay(grads, img))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sFmeZxdfJhN",
        "colab_type": "text"
      },
      "source": [
        "## Activation Maximization avec Keras Vis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__tE1ceMvAER",
        "colab_type": "text"
      },
      "source": [
        "![Texte alternatif…](https://image.slidesharecdn.com/dlcv2018d3l4interpretability-180702081944/95/interpretability-of-convolutional-neural-networks-eva-mohedano-upc-barcelona-2018-29-638.jpg?cb=1531320625)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QU4i3lptBcQE",
        "colab_type": "text"
      },
      "source": [
        "###Préparation du Modèle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfuhbbIAh9xS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.applications import VGG16\n",
        "from vis.utils import utils\n",
        "from keras import activations\n",
        "\n",
        "model = VGG16(weights='imagenet', include_top=True)\n",
        "\n",
        "\n",
        "layer_idx = utils.find_layer_idx(model, 'predictions')\n",
        "\n",
        "model.layers[layer_idx].activation = activations.linear\n",
        "model = utils.apply_modifications(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7USxluuLwcWa",
        "colab_type": "text"
      },
      "source": [
        "###Affichage de l'activation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75ZwVcc1hygT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from vis.visualization import visualize_activation\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (18, 6)\n",
        "from vis.input_modifiers import Jitter\n",
        "\n",
        "#\n",
        "\n",
        "# 20 est le code de la catégorie 'ouzel' dans imagenet\n",
        "#pour plus d'infos sur les paramétres, merci de voir le site : https://raghakot.github.io/keras-vis/\n",
        "#liste des codes de classe dans imagenet: https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a\n",
        "img = visualize_activation(model, layer_idx, filter_indices=96, max_iter=500, input_modifiers=[Jitter(16)])\n",
        "plt.imshow(img)\n",
        "\n",
        "#tester d'autres classes depuis https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a\n",
        "#96: 'toucan'\n",
        "#120: 'fiddler crab'\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7jSWWsAnTa9",
        "colab_type": "text"
      },
      "source": [
        "# Exploration de LIME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXBfWgqWQc-A",
        "colab_type": "text"
      },
      "source": [
        " Local Interpretable Model-Agnostic Explanations \n",
        "![Texte alternatif…](https://blogs.infosupport.com/wp-content/uploads/2018/07/lime.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6qYMp4hKiBM",
        "colab_type": "text"
      },
      "source": [
        "##Préparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0DVRzyvnS4Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#les sources codes proviennent du notebook https://github.com/marcotcr/lime/blob/master/doc/notebooks/Tutorial%20-%20Image%20Classification%20Keras.ipynb\n",
        "#https://github.com/marcotcr/lime/tree/master/doc/notebooks\n",
        "!pip install lime --quiet\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wnACaaYnSg1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import keras\n",
        "from keras.applications import inception_v3 as inc_net\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.imagenet_utils import decode_predictions\n",
        "from skimage.io import imread\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "print('Notebook run using keras:', keras.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4RU-Q8BoKyw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inet_model = inc_net.InceptionV3()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeauXdmRKWzb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Une fonction utilitaire pour transformer l'image utilisée\n",
        "def transform_img_fn(path_list):\n",
        "    out = []\n",
        "    for img_path in path_list:\n",
        "        img = image.load_img(img_path, target_size=(299, 299))\n",
        "        x = image.img_to_array(img)\n",
        "        x = np.expand_dims(x, axis=0)\n",
        "        x = inc_net.preprocess_input(x)\n",
        "        out.append(x)\n",
        "    return np.vstack(out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0G00GMpSKb2S",
        "colab_type": "text"
      },
      "source": [
        "##Prédiction "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUrq_ojXKe3D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images = transform_img_fn([os.path.join('IndabaxMorocco/','scorpion.jpg')])\n",
        "#images = transform_img_fn([os.path.join('IndabaxMorocco/','cat_mouse.jpg')])\n",
        "\n",
        "plt.imshow(images[0] / 2 + 0.5)\n",
        "preds = inet_model.predict(images)\n",
        "for x in decode_predictions(preds)[0]:\n",
        "    print(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mv2YMLX9Kon3",
        "colab_type": "text"
      },
      "source": [
        "##Explication"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WE4y8W7rwCFv",
        "colab_type": "text"
      },
      "source": [
        "![Texte alternatif…](https://cdn-images-1.medium.com/max/1600/1*mdnqGE3TWHbg266B9vzgOA.jpeg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbKjEawbKrIO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "import os,sys\n",
        "try:\n",
        "    import lime\n",
        "except:\n",
        "    sys.path.append(os.path.join('..', '..')) # add the current directory\n",
        "    import lime\n",
        "from lime import lime_image\n",
        "\n",
        "explainer = lime_image.LimeImageExplainer()\n",
        "#%%time\n",
        "# Création des instances perturbées : nombre 1000 et création de la courbe de régression pondérée pour trouver la perturbation qui a la plus grande pondération\n",
        "explanation = explainer.explain_instance(images[0], inet_model.predict, top_labels=5, hide_color=0, num_samples=1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0-m2ZgsLAUm",
        "colab_type": "text"
      },
      "source": [
        "Regardons l'explication de la classe Top : **scorpion**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2sSFluULIgq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#('n01770393', 'scorpion', 0.82200426)\n",
        "#('n02105251', 'briard', 0.0011861428)\n",
        "#('n02112706', 'Brabancon_griffon', 0.0010143266)\n",
        "#('n03290653', 'entertainment_center', 0.0009968795)\n",
        "#('n01692333', 'Gila_monster', 0.00084664806)\n",
        "\n",
        "from skimage.segmentation import mark_boundaries\n",
        "\n",
        "temp, mask = explanation.get_image_and_mask(71, positive_only=True, num_features=15, hide_rest=True, min_weight=0.01)\n",
        "plt.imshow(mark_boundaries(temp / 2 + 0.5, mask))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgVY4T-ALRQF",
        "colab_type": "text"
      },
      "source": [
        "Ou avec le reste de l'image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGUlqRu6LQoN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "temp, mask = explanation.get_image_and_mask(71, positive_only=True, num_features=15, hide_rest=False)\n",
        "plt.imshow(mark_boundaries(temp / 2 + 0.5, mask))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sscIAH-tLWnN",
        "colab_type": "text"
      },
      "source": [
        "Nous pouvons rajouter les pros et les cons de la classification scorpion (pros en vert, contres en rouge)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hABHs0qtLbPw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "temp, mask = explanation.get_image_and_mask(71, positive_only=False, num_features=15, hide_rest=False)\n",
        "plt.imshow(mark_boundaries(temp / 2 + 0.5, mask))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3tY4SmNiO66",
        "colab_type": "text"
      },
      "source": [
        "#Exploration de Google Lucid "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-p6dJN50RRpR",
        "colab_type": "text"
      },
      "source": [
        "depuis: https://github.com/tensorflow/lucid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkBXzYIcRBZD",
        "colab_type": "text"
      },
      "source": [
        "##Feature Invertion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85a3gqqDTmYF",
        "colab_type": "text"
      },
      "source": [
        "Reference:  [Understanding Deep Image Representations by Inverting Them](https://arxiv.org/pdf/1412.0035.pdf), Mahendran & Vedaldi, 2015\n",
        "\n",
        "![Texte alternatif…](https://camo.githubusercontent.com/d131a299baffb375706cf20c17530feed307ca08/68747470733a2f2f73746f726167652e676f6f676c65617069732e636f6d2f6c756369642d7374617469632f6d6973632f737469636b6572732f636f6c61622d666561747572652d696e76657273696f6e2e6970796e622e706e67)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwhCXRV7R4G0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install Lucid\n",
        "!pip install --quiet lucid==0.0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZlpuozWqDDyF",
        "colab": {}
      },
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import scipy.ndimage as nd\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "import lucid.modelzoo.vision_models as models\n",
        "import lucid.optvis.objectives as objectives\n",
        "import lucid.optvis.param as param\n",
        "import lucid.optvis.render as render\n",
        "import lucid.optvis.transform as transform\n",
        "from lucid.misc.io import show, load\n",
        "from lucid.misc.io.reading import read\n",
        "\n",
        "# Import the InceptionV1 (GoogLeNet) model from the Lucid modelzoo\n",
        "\n",
        "model = models.InceptionV1()\n",
        "model.load_graphdef()\n",
        "\n",
        "def imgToModelSize(arr):\n",
        "  W = model.image_shape[0]\n",
        "  w, h, _ = arr.shape\n",
        "  s = float(W) / min(w,h)\n",
        "  arr = nd.zoom(arr, [s, s, 1], mode=\"nearest\")\n",
        "  w, h, _ = arr.shape\n",
        "  dw, dh = (w-W)//2, (h-W)//3\n",
        "  return arr[dw:dw+W, dh:dh+W]\n",
        "\n",
        "@objectives.wrap_objective\n",
        "def dot_compare(layer, batch=1, cossim_pow=0):\n",
        "  def inner(T):\n",
        "    dot = tf.reduce_sum(T(layer)[batch] * T(layer)[0])\n",
        "    mag = tf.sqrt(tf.reduce_sum(T(layer)[0]**2))\n",
        "    cossim = dot/(1e-6 + mag)\n",
        "    return dot * cossim ** cossim_pow\n",
        "  return inner\n",
        "\n",
        "@objectives.wrap_objective\n",
        "def dot_compare(layer, batch=1, cossim_pow=0):\n",
        "  def inner(T):\n",
        "    dot = tf.reduce_sum(T(layer)[batch] * T(layer)[0])\n",
        "    mag = tf.sqrt(tf.reduce_sum(T(layer)[0]**2))\n",
        "    cossim = dot/(1e-6 + mag)\n",
        "    return dot * cossim ** cossim_pow\n",
        "  return inner\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsUH4PLwVTyM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def feature_inversion(img=None, layer=None, n_steps=512, cossim_pow=0.0):\n",
        "  with tf.Graph().as_default(), tf.Session() as sess:\n",
        "    img = imgToModelSize(img)\n",
        "    \n",
        "    objective = objectives.Objective.sum([\n",
        "        1.0 * dot_compare(layer, cossim_pow=cossim_pow),\n",
        "        objectives.blur_input_each_step(),\n",
        "    ])\n",
        "\n",
        "    t_input = tf.placeholder(tf.float32, img.shape)\n",
        "    param_f = param.image(img.shape[0], decorrelate=True, fft=True, alpha=False)\n",
        "    param_f = tf.stack([param_f[0], t_input])\n",
        "\n",
        "    transforms = [\n",
        "      transform.pad(8, mode='constant', constant_value=.5),\n",
        "      transform.jitter(8),\n",
        "      transform.random_scale([0.9, 0.95, 1.05, 1.1] + [1]*4),\n",
        "      transform.random_rotate(list(range(-5, 5)) + [0]*5),\n",
        "      transform.jitter(2),\n",
        "    ]\n",
        "\n",
        "    T = render.make_vis_T(model, objective, param_f, transforms=transforms)\n",
        "    loss, vis_op, t_image = T(\"loss\"), T(\"vis_op\"), T(\"input\")\n",
        "\n",
        "    tf.global_variables_initializer().run()\n",
        "    for i in range(n_steps): _ = sess.run([vis_op], {t_input: img})\n",
        "\n",
        "    result = t_image.eval(feed_dict={t_input: img})\n",
        "    show(result[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVSW8XIdRkK2",
        "colab_type": "text"
      },
      "source": [
        "##Execution sur toutes les couches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyVkCaoeRsVX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img = load(\"https://storage.googleapis.com/lucid-static/building-blocks/examples/dog_cat.png\")\n",
        "\n",
        "\n",
        "figure(figsize=(10, 10), dpi=100)\n",
        "imgplot = mpimg.imread('https://storage.googleapis.com/lucid-static/building-blocks/examples/dog_cat.png')\n",
        "plt.imshow(imgplot)\n",
        "plt.grid(False)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "    \n",
        "\n",
        "\n",
        "#layers = ['conv2d%d' % i for i in range(0, 3)] + ['mixed3a', 'mixed3b', \n",
        "#                                                  'mixed4a', 'mixed4b', 'mixed4c', 'mixed4d', 'mixed4e',\n",
        "#                                                  'mixed5a', 'mixed5b']\n",
        "layers = ['conv2d%d' % i for i in range(0, 3)] + ['mixed3a', 'mixed3b']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCYnKYJYUbB9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for layer in layers:\n",
        "  print(layer)\n",
        "  feature_inversion(img, layer=layer)\n",
        "  print ()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0crj8VMJiZwf",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBYLKqzSiHO1",
        "colab_type": "text"
      },
      "source": [
        "#Exploration de Deep Explain\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHXsQrCddSib",
        "colab_type": "text"
      },
      "source": [
        "##Installation et prédiction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Fjxv8pW1nA3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Installation de DeepExplain  pour utiliser LRP \n",
        "# A unified framework of perturbation and gradient-based attribution methods for Deep Neural Networks interpretability. \n",
        "!pip install git+https://github.com/marcoancona/DeepExplain.git#egg=deepexplain"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9EWUD-TeMX2",
        "colab_type": "text"
      },
      "source": [
        "###Fonctions utiles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3u05goui3ImV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Quelques fonctions proviennent de https://colab.research.google.com/github/idealo/cnn-exposed/blob/master/notebooks/Attribution.ipynb#scrollTo=U0rHlH4AnQPV\n",
        "from deepexplain.tensorflow import DeepExplain\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import tempfile, sys, os\n",
        "sys.path.insert(0, os.path.abspath('..'))\n",
        "\n",
        "import numpy as np\n",
        "from scipy.misc import imread\n",
        "import tensorflow as tf\n",
        "from tensorflow.contrib.slim.nets import inception\n",
        "\n",
        "slim = tf.contrib.slim\n",
        "    \n",
        "from deepexplain.tensorflow import DeepExplain\n",
        "\n",
        "from skimage import feature, transform\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# import keras dependencies\n",
        "from keras.models import Model\n",
        "from keras.applications import MobileNet as CNN\n",
        "from keras.applications.mobilenet import preprocess_input, decode_predictions\n",
        "from keras.preprocessing import image\n",
        "from keras.utils import to_categorical\n",
        "import keras.backend as K\n",
        "\n",
        "\n",
        "# import specific functions from keras-vis package\n",
        "from vis.utils import utils\n",
        "from vis.visualization import visualize_cam, overlay\n",
        "\n",
        "    \n",
        "def plot_single_image(image_path, fig_size=(10, 10), dpi=100):\n",
        "    figure(figsize=fig_size, dpi=dpi)\n",
        "    img = mpimg.imread(image_path)\n",
        "    plt.imshow(img)\n",
        "    plt.grid(False)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "    \n",
        "\n",
        "# Plotting function for saliency maps\n",
        "def plot_custom(data, xi=None, cmap='RdBu_r', axis=plt, percentile=100, dilation=3.0, alpha=0.8):\n",
        "    dx, dy = 0.05, 0.05\n",
        "    xx = np.arange(0.0, data.shape[1], dx)\n",
        "    yy = np.arange(0.0, data.shape[0], dy)\n",
        "    xmin, xmax, ymin, ymax = np.amin(xx), np.amax(xx), np.amin(yy), np.amax(yy)\n",
        "    extent = xmin, xmax, ymin, ymax\n",
        "    cmap_xi = plt.get_cmap('Greys_r')\n",
        "    cmap_xi.set_bad(alpha=0)\n",
        "    overlay = None\n",
        "    if xi is not None:\n",
        "        # Compute edges (to overlay to heatmaps later)\n",
        "        xi_greyscale = xi if len(xi.shape) == 2 else np.mean(xi, axis=-1)\n",
        "        in_image_upscaled = transform.rescale(xi_greyscale, dilation, mode='constant')\n",
        "        edges = feature.canny(in_image_upscaled).astype(float)\n",
        "        edges[edges < 0.5] = np.nan\n",
        "        edges[:5, :] = np.nan\n",
        "        edges[-5:, :] = np.nan\n",
        "        edges[:, :5] = np.nan\n",
        "        edges[:, -5:] = np.nan\n",
        "        overlay = edges\n",
        "\n",
        "    abs_max = np.percentile(np.abs(data), percentile)\n",
        "    abs_min = -abs_max\n",
        "\n",
        "    if len(data.shape) == 3:\n",
        "        data = np.mean(data, 2)\n",
        "    axis.imshow(data, extent=extent, interpolation='bicubic', cmap=cmap, vmin=abs_min, vmax=abs_max)\n",
        "    if overlay is not None:\n",
        "        axis.imshow(overlay, extent=extent, interpolation='bicubic', cmap=cmap_xi, alpha=alpha)\n",
        "    axis.axis('off')\n",
        "    return axis\n",
        "\n",
        "\n",
        "def plot_comparison(target_image_path, map_array, title=''):\n",
        "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(26, 20))\n",
        "\n",
        "    img_orig = Image.open(target_image_path).resize((224, 224))\n",
        "    xi = (map_array[0,:] - np.min(map_array[0,:]))\n",
        "    xi /= np.max(xi)\n",
        "\n",
        "    ax = axes.flatten()[0]\n",
        "    ax.imshow(img_orig)\n",
        "    ax.set_title('Original', fontdict={'fontsize': 20})\n",
        "    ax.axis('off')\n",
        "\n",
        "    plot_custom(attributions[0], xi = xi, axis=axes[1], dilation=.5, percentile=99, alpha=.2).set_title(title, fontdict={'fontsize': 20})\n",
        "    plt.show()\n",
        "\n",
        "    #from https://raw.githubusercontent.com/marcoancona/DeepExplain/master/examples/utils.py\n",
        "\n",
        "\n",
        "def plot(data, xi=None, cmap='RdBu_r', axis=plt, percentile=100, dilation=3.0, alpha=0.8):\n",
        "    dx, dy = 0.05, 0.05\n",
        "    xx = np.arange(0.0, data.shape[1], dx)\n",
        "    yy = np.arange(0.0, data.shape[0], dy)\n",
        "    xmin, xmax, ymin, ymax = np.amin(xx), np.amax(xx), np.amin(yy), np.amax(yy)\n",
        "    extent = xmin, xmax, ymin, ymax\n",
        "    cmap_xi = plt.get_cmap('Greys_r')\n",
        "    cmap_xi.set_bad(alpha=0)\n",
        "    overlay = None\n",
        "    if xi is not None:\n",
        "        # Compute edges (to overlay to heatmaps later)\n",
        "        xi_greyscale = xi if len(xi.shape) == 2 else np.mean(xi, axis=-1)\n",
        "        in_image_upscaled = transform.rescale(xi_greyscale, dilation, mode='constant')\n",
        "        edges = feature.canny(in_image_upscaled).astype(float)\n",
        "        edges[edges < 0.5] = np.nan\n",
        "        edges[:5, :] = np.nan\n",
        "        edges[-5:, :] = np.nan\n",
        "        edges[:, :5] = np.nan\n",
        "        edges[:, -5:] = np.nan\n",
        "        overlay = edges\n",
        "\n",
        "    abs_max = np.percentile(np.abs(data), percentile)\n",
        "    abs_min = abs_max\n",
        "\n",
        "    if len(data.shape) == 3:\n",
        "        data = np.mean(data, 2)\n",
        "    axis.imshow(data, extent=extent, interpolation='none', cmap=cmap, vmin=-abs_min, vmax=abs_max)\n",
        "    if overlay is not None:\n",
        "        axis.imshow(overlay, extent=extent, interpolation='none', cmap=cmap_xi, alpha=alpha)\n",
        "    axis.axis('off')\n",
        "    return axis"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jytbgYFeVx_",
        "colab_type": "text"
      },
      "source": [
        "###Prédiction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPVAdCEr6vYF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "path_resources = pathlib.Path('IndabaxMorocco/')\n",
        "target_image = os.path.join(path_resources, 'snake.png')\n",
        "plot_single_image(target_image)\n",
        "\n",
        "model = CNN(include_top=True)\n",
        "\n",
        "loaded_image = np.array(image.load_img(target_image, target_size=(224, 224)))\n",
        "processed_image = preprocess_input(loaded_image)\n",
        "preds = model.predict(processed_image[np.newaxis, :])\n",
        "preds_name = decode_predictions(preds, top=3)\n",
        "preds_name\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cN4D2jpXehOE",
        "colab_type": "text"
      },
      "source": [
        "##Création des cartes d'attributions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2i3qrFmw2ahf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import deepexplain to draw saliency maps\n",
        "from deepexplain.tensorflow import DeepExplain\n",
        "top_idx = preds.argsort()[::-1],\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nITrS85y2eUt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import deepexplain to draw saliency maps\n",
        "from deepexplain.tensorflow import DeepExplain\n",
        "# Get saliency map\n",
        "# Refer the API documentation for using deepexplain as below\n",
        "with DeepExplain(session=K.get_session()) as de:\n",
        "    model = CNN(include_top=True)\n",
        "    input_tensor = model.layers[0].input\n",
        "    fModel = Model(inputs=input_tensor, outputs = model.layers[-1].output)\n",
        "    target_tensor = fModel(input_tensor)\n",
        "    top_idx = preds.argsort()[::-1]\n",
        "    ys = to_categorical(top_idx, num_classes=1000)\n",
        "    xs = np.tile(processed_image, (1, 1, 1, 1))\n",
        "    attributions = {\n",
        "            'Saliency'         :de.explain('saliency', fModel.outputs[0] * ys, fModel.inputs[0], xs),\n",
        "            'Occlusion [15x15]':    de.explain('occlusion', fModel.outputs[0] * ys, fModel.inputs[0], xs, window_shape=(15,15,3), step=4),\n",
        "            'Gradient * Input':     de.explain('grad*input', fModel.outputs[0] * ys, fModel.inputs[0], xs),\n",
        "            'Integrated Gradients': de.explain('intgrad', fModel.outputs[0] * ys, fModel.inputs[0], xs),\n",
        "            'Epsilon-LRP':          de.explain('elrp', fModel.outputs[0] * ys, fModel.inputs[0], xs),\n",
        "   }  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJKpFB2hgunV",
        "colab_type": "text"
      },
      "source": [
        "##Affichage des résultats"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WQJUGV2AQzT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot attributions\n",
        "#from utils import plot, plt\n",
        "#import matplotlib as mpl\n",
        "#import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "n_cols = int(len(attributions)) + 1\n",
        "n_rows = len(xs) \n",
        "fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(3*n_cols, 3*n_rows))\n",
        "\n",
        "for i, xi in enumerate(xs):\n",
        "    xi = (xi - np.min(xi))\n",
        "    xi /= np.max(xi)\n",
        "    ax = axes.flatten()[i*n_cols]\n",
        "    ax.imshow(xi)\n",
        "    ax.set_title('Original')\n",
        "    ax.axis('off')\n",
        "    for j, a in enumerate(attributions):\n",
        "        axj = axes.flatten()[i*n_cols + j + 1]\n",
        "        plot(attributions[a][i], xi = xi, axis=axj, dilation=.5, percentile=99, alpha=.2).set_title(a)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iqkg9OMXfAic",
        "colab_type": "text"
      },
      "source": [
        "A partir du notebook de site officiel Lucid https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/tutorial.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQXKRFHki60v",
        "colab_type": "text"
      },
      "source": [
        "# Miscellaneous"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8REDYiDujgjr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwbS-2YEi8j1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from vis.visualization import get_num_filters\n",
        "\n",
        "# The name of the layer we want to visualize\n",
        "# You can see this in the model definition.\n",
        "layer_name = 'block1_conv1'\n",
        "layer_idx = utils.find_layer_idx(model, layer_name)\n",
        "\n",
        "# Visualize all filters in this layer.\n",
        "filters = np.arange(get_num_filters(model.layers[layer_idx]))\n",
        "\n",
        "# Generate input image for each filter.\n",
        "vis_images = []\n",
        "for idx in filters:\n",
        "    img = visualize_activation(model, layer_idx, filter_indices=idx)\n",
        "    \n",
        "    # Utility to overlay text on image.\n",
        "    img = utils.draw_text(img, 'Filter {}'.format(idx))    \n",
        "    vis_images.append(img)\n",
        "\n",
        "# Generate stitched image palette with 8 cols.\n",
        "stitched = utils.stitch_images(vis_images, cols=8)    \n",
        "plt.axis('off')\n",
        "plt.imshow(stitched)\n",
        "plt.title(layer_name)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}